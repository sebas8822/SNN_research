{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41151e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic PMAT Patch Generator — **uses your project root**\n",
    "# This notebook **uses your folder layout and variable names** to generate *training-only* synthetic patches for PMAT classes, with optional ControlNet (Seg) conditioning. It avoids leakage by only using **train annotations**.\n",
    "\n",
    "# **Outputs (relative to your repo root):**\n",
    "# - `synthetic_patches/<Class>/...` — New images\n",
    "# - `artifacts/synth_manifest.json` — Provenance (class, source patch, seed, method)\n",
    "# - Optional preview grids\n",
    "\n",
    "# > Toggle `MODE = \"DIFFUSION\"` to use Stable Diffusion + ControlNet (seg). Keep it `\"MOCK\"` if you just want the wiring verified first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f48d84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using projection: mean_projection\n",
      "Repo path: /home/sebas_dev_linux/projects/snn_project/Diffusion_SNN_ML_v0/borg-main\n",
      "Data root: /home/sebas_dev_linux/projects/snn_project/Diffusion_SNN_ML_v0/borg-main/data/mean_projection\n",
      "Train images: /home/sebas_dev_linux/projects/snn_project/Diffusion_SNN_ML_v0/borg-main/data/mean_projection/images/train\n",
      "Val images: /home/sebas_dev_linux/projects/snn_project/Diffusion_SNN_ML_v0/borg-main/data/mean_projection/images/val\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 — Config & root discovery\n",
    "import os, json, random, glob\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "from PIL import Image, ImageOps, ImageFilter\n",
    "\n",
    "# --- Choose projection folder to use ---\n",
    "PROJECTION = \"mean_projection\"   # change to \"max_projection\" if you prefer\n",
    "project_path = \".\"\n",
    "repo_path = os.path.join(project_path, \"borg-main\")\n",
    "data_root_dir = os.path.join(repo_path, f\"data/{PROJECTION}\")\n",
    "train_json_path = os.path.join(data_root_dir, \"organoid_coco_train.json\")\n",
    "val_json_path   = os.path.join(data_root_dir, \"organoid_coco_val.json\")\n",
    "train_images_dir = os.path.join(data_root_dir, \"images\", \"train\")\n",
    "val_images_dir   = os.path.join(data_root_dir, \"images\", \"val\")\n",
    "\n",
    "# Output dirs\n",
    "PATCH_CACHE_TRAIN = \"patch_cache_train\"   # new cache for cropped patches (train only)\n",
    "MASKS_DIR         = \"processed_masks_train\"\n",
    "SYNTH_DIR         = \"synthetic_patches\"\n",
    "ARTIFACTS         = \"artifacts\"\n",
    "\n",
    "IMG_SIZE = 96\n",
    "CLASSES = [\"Prophase\",\"Metaphase\",\"Anaphase\",\"Telophase\"]\n",
    "\n",
    "for p in [PATCH_CACHE_TRAIN, MASKS_DIR, SYNTH_DIR, ARTIFACTS]:\n",
    "    Path(p).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Using projection:\", PROJECTION)\n",
    "print(\"Repo path:\", os.path.abspath(repo_path))\n",
    "print(\"Data root:\", os.path.abspath(data_root_dir))\n",
    "print(\"Train images:\", os.path.abspath(train_images_dir))\n",
    "print(\"Val images:\", os.path.abspath(val_images_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "996d21c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: {1: 'Prophase', 2: 'Metaphase', 3: 'Anaphase', 4: 'Telophase'}\n",
      "Train annotations: 556\n",
      "Val annotations: 181\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 — Load train/val annotations (COCO JSON)\n",
    "with open(train_json_path, \"r\") as f:\n",
    "    train_coco = json.load(f)\n",
    "with open(val_json_path, \"r\") as f:\n",
    "    val_coco = json.load(f)\n",
    "\n",
    "categories_map: Dict[int, str] = {c[\"id\"]: c[\"name\"] for c in train_coco[\"categories\"]}\n",
    "train_id_to_filename = {img[\"id\"]: img[\"file_name\"] for img in train_coco[\"images\"]}\n",
    "val_id_to_filename   = {img[\"id\"]: img[\"file_name\"] for img in val_coco[\"images\"]}\n",
    "\n",
    "train_annotations = train_coco[\"annotations\"]   # we will synthesize from these only\n",
    "val_annotations   = val_coco[\"annotations\"]\n",
    "\n",
    "print(\"Classes:\", categories_map)\n",
    "print(\"Train annotations:\", len(train_annotations))\n",
    "print(\"Val annotations:\", len(val_annotations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef0c533d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached train patches: 556\n",
      "Per-class cached: {'Prophase': 282, 'Metaphase': 146, 'Anaphase': 69, 'Telophase': 59}\n",
      "Missing source images: 0 | Skipped (bad/degenerate bbox): 0\n",
      "Example file_name from JSON: images/train/phase_1_new_v10_frame_02.png\n",
      "Resolved path: borg-main/data/mean_projection/images/train/phase_1_new_v10_frame_02.png\n"
     ]
    }
   ],
   "source": [
    "# Cell 3 — Cache train patches directly from COCO bboxes (robust path resolution)\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "# Ensure cache dirs exist\n",
    "for cls in CLASSES:\n",
    "    Path(PATCH_CACHE_TRAIN, cls).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def resolve_image_path(file_name: str) -> Path | None:\n",
    "    \"\"\"Resolve image path regardless of whether file_name already contains images/train/…\"\"\"\n",
    "    p = Path(file_name)\n",
    "    cands = []\n",
    "    if p.is_absolute():\n",
    "        cands.append(p)\n",
    "    # Typical COCO: file_name already 'images/train/xxx.png' relative to the projection root\n",
    "    cands.append(Path(data_root_dir) / file_name)\n",
    "    # Fallbacks: try train-images dir and basename-only\n",
    "    cands.append(Path(train_images_dir) / file_name)\n",
    "    cands.append(Path(train_images_dir) / p.name)\n",
    "    for c in cands:\n",
    "        if c.exists():\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def _si(v):  # safe int\n",
    "    try:\n",
    "        return int(round(float(v)))\n",
    "    except Exception:\n",
    "        return int(v)\n",
    "\n",
    "PAD_FRAC = 0.10  # small padding around bbox\n",
    "ann_to_patch = {}\n",
    "class_to_train_ann_ids = defaultdict(list)\n",
    "missing_imgs, skipped_bad_bbox = 0, 0\n",
    "\n",
    "for ann in train_annotations:\n",
    "    cls = categories_map[ann[\"category_id\"]]\n",
    "    img_id = ann[\"image_id\"]\n",
    "    # Look up file_name via the IMAGES table\n",
    "    file_name = train_id_to_filename.get(img_id)\n",
    "    if file_name is None:\n",
    "        missing_imgs += 1\n",
    "        continue\n",
    "    img_path = resolve_image_path(file_name)\n",
    "    if img_path is None:\n",
    "        missing_imgs += 1\n",
    "        continue\n",
    "\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    W, H = img.width, img.height\n",
    "\n",
    "    # COCO bbox: [x, y, width, height] (xywh, pixels)\n",
    "    bb = ann.get(\"bbox\")\n",
    "    if not bb or len(bb) != 4:\n",
    "        skipped_bad_bbox += 1\n",
    "        continue\n",
    "    x, y, bw, bh = float(bb[0]), float(bb[1]), float(bb[2]), float(bb[3])\n",
    "    if bw <= 1 or bh <= 1:\n",
    "        skipped_bad_bbox += 1\n",
    "        continue\n",
    "\n",
    "    # pad and clamp to image bounds\n",
    "    x0 = max(0, _si(x - PAD_FRAC * bw))\n",
    "    y0 = max(0, _si(y - PAD_FRAC * bh))\n",
    "    x1 = min(W, _si(x + bw + PAD_FRAC * bw))\n",
    "    y1 = min(H, _si(y + bh + PAD_FRAC * bh))\n",
    "    if x1 <= x0 or y1 <= y0:\n",
    "        skipped_bad_bbox += 1\n",
    "        continue\n",
    "\n",
    "    patch = img.crop((x0, y0, x1, y1)).resize((IMG_SIZE, IMG_SIZE), Image.Resampling.LANCZOS)\n",
    "    outp = Path(PATCH_CACHE_TRAIN) / cls / f\"patch_{ann['id']}.png\"\n",
    "    patch.save(outp)\n",
    "    ann_to_patch[ann[\"id\"]] = str(outp)\n",
    "    class_to_train_ann_ids[cls].append(ann[\"id\"])\n",
    "\n",
    "print(\"Cached train patches:\", len(ann_to_patch))\n",
    "print(\"Per-class cached:\", {c: len(v) for c, v in class_to_train_ann_ids.items()})\n",
    "print(\"Missing source images:\", missing_imgs, \"| Skipped (bad/degenerate bbox):\", skipped_bad_bbox)\n",
    "\n",
    "# Optional quick sanity check: print a couple of resolved examples\n",
    "some = next(iter(train_annotations), None)\n",
    "if some:\n",
    "    fn = train_id_to_filename.get(some[\"image_id\"])\n",
    "    print(\"Example file_name from JSON:\", fn)\n",
    "    print(\"Resolved path:\", resolve_image_path(fn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "450b9b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_72265/3235379901.py:18: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
      "  m_img = Image.fromarray(m, mode='L').filter(ImageFilter.MaxFilter(3)).filter(ImageFilter.MinFilter(3))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masks written: 556\n"
     ]
    }
   ],
   "source": [
    "# Cell 4 — Build masks from processed patches (robust Otsu + closing). \n",
    "# If you have Cellpose/Ilastik masks, plug them here instead.\n",
    "import numpy as np\n",
    "from PIL import ImageFilter, Image\n",
    "\n",
    "MASKS_DIR = 'processed_masks_train'  # predicted/derived masks for train only\n",
    "Path(MASKS_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def mask_from_patch(png_path: str) -> Image.Image:\n",
    "    img = Image.open(png_path).convert('RGB').resize((IMG_SIZE, IMG_SIZE), Image.Resampling.LANCZOS)\n",
    "    # Use red+green emphasis (your dataset is dual-channel fluorescence; nucleus often brighter in R)\n",
    "    arr = np.array(img).astype(np.uint8)\n",
    "    # Weighted grayscale favoring R, then Otsu threshold\n",
    "    gray = (0.6*arr[...,0] + 0.3*arr[...,1] + 0.1*arr[...,2]).astype(np.uint8)\n",
    "    # Otsu threshold\n",
    "    th = gray.mean() + 0.5*gray.std()   # fallback if cv2 not available; simple adaptive rule\n",
    "    m = (gray > th).astype(np.uint8)*255\n",
    "    m_img = Image.fromarray(m, mode='L').filter(ImageFilter.MaxFilter(3)).filter(ImageFilter.MinFilter(3))\n",
    "    return m_img\n",
    "\n",
    "# Write masks for train patches\n",
    "for cls in CLASSES:\n",
    "    (Path(MASKS_DIR)/cls).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "wrote = 0\n",
    "for cls, ann_ids in class_to_train_ann_ids.items():\n",
    "    for ann_id in ann_ids:\n",
    "        src = ann_to_patch[ann_id]\n",
    "        outp = Path(MASKS_DIR)/cls/f\"mask_{ann_id}.png\"\n",
    "        if not outp.exists():\n",
    "            m = mask_from_patch(src)\n",
    "            m.save(outp)\n",
    "            wrote += 1\n",
    "print(\"Masks written:\", wrote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c63f2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sebas_dev_linux/miniconda3/envs/diff/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading pipeline components...: 100%|██████████| 6/6 [00:03<00:00,  1.72it/s]\n",
      "You have disabled the safety checker for <class 'diffusers.pipelines.controlnet.pipeline_controlnet.StableDiffusionControlNetPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StableDiffusionControlNetPipeline {\n",
       "  \"_class_name\": \"StableDiffusionControlNetPipeline\",\n",
       "  \"_diffusers_version\": \"0.35.2\",\n",
       "  \"_name_or_path\": \"runwayml/stable-diffusion-v1-5\",\n",
       "  \"controlnet\": [\n",
       "    \"diffusers\",\n",
       "    \"ControlNetModel\"\n",
       "  ],\n",
       "  \"feature_extractor\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPImageProcessor\"\n",
       "  ],\n",
       "  \"image_encoder\": [\n",
       "    null,\n",
       "    null\n",
       "  ],\n",
       "  \"requires_safety_checker\": true,\n",
       "  \"safety_checker\": [\n",
       "    null,\n",
       "    null\n",
       "  ],\n",
       "  \"scheduler\": [\n",
       "    \"diffusers\",\n",
       "    \"UniPCMultistepScheduler\"\n",
       "  ],\n",
       "  \"text_encoder\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTextModel\"\n",
       "  ],\n",
       "  \"tokenizer\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTokenizer\"\n",
       "  ],\n",
       "  \"unet\": [\n",
       "    \"diffusers\",\n",
       "    \"UNet2DConditionModel\"\n",
       "  ],\n",
       "  \"vae\": [\n",
       "    \"diffusers\",\n",
       "    \"AutoencoderKL\"\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
    "\n",
    "controlnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-seg\", torch_dtype=dtype)\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    controlnet=controlnet,\n",
    "    safety_checker=None,\n",
    "    torch_dtype=dtype,\n",
    ")\n",
    "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "pipe.enable_attention_slicing()   # VRAM saver (use this instead of xFormers)\n",
    "pipe.enable_vae_slicing()\n",
    "pipe.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "294dc86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 6/6 [00:00<00:00, 29.67it/s]\n",
      "You have disabled the safety checker for <class 'diffusers.pipelines.controlnet.pipeline_controlnet.StableDiffusionControlNetPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded SD1.5 + ControlNet(seg) on cuda\n"
     ]
    }
   ],
   "source": [
    "# Cell 5 — Diffusion scaffolding (ControlNet Seg) OR MOCK texture generator\n",
    "MODE = \"DIFFUSION\" #\"MOCK\"   # change to \"DIFFUSION\" to enable Stable Diffusion + ControlNet(seg)\n",
    "\n",
    "PROMPTS = {\n",
    "    \"Prophase\":  \"fluorescence microscopy, nucleus in prophase, confocal, cellular texture\",\n",
    "    \"Metaphase\": \"fluorescence microscopy, metaphase, chromosomes aligned at the equatorial plate\",\n",
    "    \"Anaphase\":  \"fluorescence microscopy, anaphase, chromosomes to opposite poles\",\n",
    "    \"Telophase\": \"fluorescence microscopy, telophase, two daughter nuclei forming\",\n",
    "}\n",
    "NEGATIVE = \"cartoon, text, watermark, artifacts, blurry, out of focus\"\n",
    "\n",
    "pipe = None\n",
    "device = None\n",
    "\n",
    "def try_load_diffusion():\n",
    "    try:\n",
    "        from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\n",
    "        import torch\n",
    "    except Exception as e:\n",
    "        print(\"Diffusers not available; remaining in MOCK mode.\\\\n\", e)\n",
    "        return None, None\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    controlnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-seg\")\n",
    "    pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "        \"runwayml/stable-diffusion-v1-5\",\n",
    "        controlnet=controlnet,\n",
    "        safety_checker=None,\n",
    "    )\n",
    "    pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "    pipe = pipe.to(device)\n",
    "    print(f\"Loaded SD1.5 + ControlNet(seg) on {device}\")\n",
    "    return pipe, device\n",
    "\n",
    "def generate_with_controlnet(pipe, device, rgb_img: Image.Image, seg_mask: Image.Image, prompt: str,\n",
    "                             strength=0.55, guidance_scale=6.5, steps=24, seed=None):\n",
    "    import torch\n",
    "    if seed is None:\n",
    "        seed = random.randint(0, 1_000_000)\n",
    "    g = torch.Generator(device=device).manual_seed(seed)\n",
    "    rgb512 = rgb_img.resize((512,512), Image.Resampling.BICUBIC)\n",
    "    seg512 = seg_mask.resize((512,512), Image.Resampling.NEAREST).convert('RGB')\n",
    "    out = pipe(\n",
    "        prompt=prompt, image=rgb512, control_image=seg512,\n",
    "        negative_prompt=NEGATIVE, generator=g,\n",
    "        num_inference_steps=steps, guidance_scale=guidance_scale, strength=strength,\n",
    "    )\n",
    "    im = out.images[0].resize((IMG_SIZE, IMG_SIZE), Image.Resampling.LANCZOS)\n",
    "    return im, {\"seed\": seed, \"steps\": steps, \"guidance_scale\": guidance_scale, \"strength\": strength}\n",
    "\n",
    "# MOCK generator — no internet/GPU required\n",
    "import numpy as np\n",
    "def mock_generate_from_mask(mask_img: Image.Image, seed=None) -> Image.Image:\n",
    "    r = random.Random(seed or random.randint(0, 999_999))\n",
    "    m = mask_img if mask_img.mode == \"L\" else mask_img.convert(\"L\")\n",
    "    base = Image.new(\"RGB\", m.size, (r.randint(0,25),)*3)\n",
    "    # noise texture\n",
    "    tex = Image.effect_noise(m.size, r.randint(30,90)).convert(\"L\")\n",
    "    tex = tex.filter(ImageFilter.GaussianBlur(radius=r.uniform(0.8, 1.6)))\n",
    "    color = Image.new(\"RGB\", m.size, (r.randint(80,240), r.randint(80,240), r.randint(80,240)))\n",
    "    color = Image.blend(color, Image.new(\"RGB\", m.size, (r.randint(0,40),)*3), r.uniform(0.2, 0.5))\n",
    "    arr_c = np.array(color).astype(np.int16)\n",
    "    arr_t = np.array(tex).astype(np.float32) / 255.0\n",
    "    arr_c = np.clip(arr_c * (0.7 + 0.6*arr_t[...,None]), 0, 255).astype(np.uint8)\n",
    "    tex_color = Image.fromarray(arr_c, mode=\"RGB\")\n",
    "    m_blur = m.filter(ImageFilter.GaussianBlur(radius=0.8))\n",
    "    out = Image.composite(tex_color, base, m_blur)\n",
    "    # gentle halo\n",
    "    edge = m.filter(ImageFilter.FIND_EDGES).filter(ImageFilter.GaussianBlur(radius=0.7))\n",
    "    edge_alpha = ImageOps.autocontrast(edge).point(lambda x: int(x*0.15))\n",
    "    overlay_rgba = Image.new(\"RGBA\", m.size, (255,255,255,0)); overlay_rgba.putalpha(edge_alpha)\n",
    "    out_rgba = out.convert(\"RGBA\")\n",
    "    final = Image.alpha_composite(out_rgba, overlay_rgba).convert(\"RGB\")\n",
    "    return final\n",
    "\n",
    "if MODE.upper().startswith(\"DIFF\"):\n",
    "    pipe, device = try_load_diffusion()\n",
    "else:\n",
    "    print(\"MODE=MOCK — using texture-based generator for quick wiring test.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acec1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train counts: {'Prophase': 282, 'Metaphase': 146, 'Anaphase': 69, 'Telophase': 59} => target per class: 282\n",
      "Prophase: already at target, skipping generation.\n",
      "Metaphase: generating 136 images…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [08:19<00:00, 20.82s/it]\n",
      "100%|██████████| 24/24 [08:17<00:00, 20.73s/it]\n",
      "100%|██████████| 24/24 [08:17<00:00, 20.72s/it]\n",
      "100%|██████████| 24/24 [08:16<00:00, 20.69s/it]\n",
      "100%|██████████| 24/24 [08:26<00:00, 21.11s/it]\n",
      "100%|██████████| 24/24 [08:19<00:00, 20.80s/it]\n",
      "100%|██████████| 24/24 [08:09<00:00, 20.42s/it]\n",
      "100%|██████████| 24/24 [08:08<00:00, 20.35s/it]\n",
      "100%|██████████| 24/24 [08:08<00:00, 20.36s/it]\n",
      "100%|██████████| 24/24 [10:30<00:00, 26.29s/it]\n",
      "100%|██████████| 24/24 [11:49<00:00, 29.57s/it]\n",
      "100%|██████████| 24/24 [11:44<00:00, 29.34s/it]\n",
      "100%|██████████| 24/24 [12:58<00:00, 32.44s/it]\n",
      "100%|██████████| 24/24 [22:48<00:00, 57.03s/it]\n",
      " 25%|██▌       | 6/24 [05:38<16:39, 55.52s/it]"
     ]
    }
   ],
   "source": [
    "# Cell 6 — Synthesize per class (train-only), with target counts and manifest\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Choose a per-class target: bring all classes up to the max train count\n",
    "train_counts = {cls: len(class_to_train_ann_ids[cls]) for cls in CLASSES}\n",
    "target = max(train_counts.values())\n",
    "print(\"Train counts:\", train_counts, \"=> target per class:\", target)\n",
    "\n",
    "manifest = []\n",
    "for cls in CLASSES:\n",
    "    outd = Path(SYNTH_DIR)/cls\n",
    "    outd.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Iterate ann_ids, generate as many as needed to reach target\n",
    "for cls, ann_ids in class_to_train_ann_ids.items():\n",
    "    need = max(0, target - len(ann_ids))\n",
    "    if need == 0:\n",
    "        print(f\"{cls}: already at target, skipping generation.\")\n",
    "        continue\n",
    "    print(f\"{cls}: generating {need} images…\")\n",
    "    # round-robin over existing train patches\n",
    "    k = 0\n",
    "    while k < need:\n",
    "        for ann_id in ann_ids:\n",
    "            if k >= need: break\n",
    "            src_patch = ann_to_patch[ann_id]\n",
    "            mask_path = Path(MASKS_DIR)/cls/f\"mask_{ann_id}.png\"\n",
    "            if not Path(src_patch).exists() or not mask_path.exists():\n",
    "                continue\n",
    "            rgb = Image.open(src_patch).convert('RGB').resize((IMG_SIZE, IMG_SIZE), Image.Resampling.BICUBIC)\n",
    "            msk = Image.open(mask_path).convert('L').resize((IMG_SIZE, IMG_SIZE), Image.Resampling.NEAREST)\n",
    "            if MODE.upper().startswith(\"DIFF\") and pipe is not None:\n",
    "                out_img, meta = generate_with_controlnet(pipe, device, rgb, msk, PROMPTS[cls])\n",
    "                method = \"controlnet_seg\"\n",
    "                seed = meta[\"seed\"]\n",
    "            else:\n",
    "                seed = 10_000 + k\n",
    "                out_img = mock_generate_from_mask(msk, seed=seed)\n",
    "                method = \"mock_from_mask\"\n",
    "            out_name = f\"{cls.lower()}_synth_{ann_id}_{k:04d}.png\"\n",
    "            out_path = Path(SYNTH_DIR)/cls/out_name\n",
    "            out_img.save(out_path)\n",
    "            manifest.append({\n",
    "                \"class\": cls,\n",
    "                \"file\": str(out_path),\n",
    "                \"source_patch\": src_patch,\n",
    "                \"source_mask\": str(mask_path),\n",
    "                \"method\": method,\n",
    "                \"seed\": seed,\n",
    "                \"img_size\": IMG_SIZE,\n",
    "            })\n",
    "            k += 1\n",
    "\n",
    "# Save manifest\n",
    "man_path = Path(ARTIFACTS)/\"synth_manifest.json\"\n",
    "with open(man_path, \"w\") as f:\n",
    "    json.dump(manifest, f, indent=2)\n",
    "print(\"Wrote manifest:\", man_path, \"| total synth:\", len(manifest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d48aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7 — Light de-dup (aHash) and class counts report\n",
    "import numpy as np\n",
    "\n",
    "def average_hash(img: Image.Image, hash_size=8):\n",
    "    g = ImageOps.grayscale(img).resize((hash_size, hash_size), Image.Resampling.LANCZOS)\n",
    "    arr = np.array(g).astype(np.float32)\n",
    "    th = arr.mean()\n",
    "    bits = (arr > th).astype(np.uint8).flatten()\n",
    "    h = 0\n",
    "    for b in bits:\n",
    "        h = (h << 1) | int(b)\n",
    "    return h\n",
    "\n",
    "def hamming(a, b):\n",
    "    return bin(a ^ b).count(\"1\")\n",
    "\n",
    "removed = 0\n",
    "for cls in CLASSES:\n",
    "    files = sorted((Path(SYNTH_DIR)/cls).glob(\"*.png\"))\n",
    "    seen = []\n",
    "    for fp in files:\n",
    "        img = Image.open(fp)\n",
    "        ah = average_hash(img)\n",
    "        if any(hamming(ah, s) <= 2 for s in seen):\n",
    "            fp.unlink(missing_ok=True)\n",
    "            removed += 1\n",
    "        else:\n",
    "            seen.append(ah)\n",
    "print(\"Removed near-duplicates:\", removed)\n",
    "\n",
    "counts = {cls: len(list((Path(SYNTH_DIR)/cls).glob('*.png'))) for cls in CLASSES}\n",
    "print(\"Synthetic counts (after dedup):\", counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d864065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8 — Preview grid (first few per class)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(len(CLASSES), 4, figsize=(6, 6))\n",
    "for r, cls in enumerate(CLASSES):\n",
    "    files = sorted(glob.glob(str((Path(SYNTH_DIR) / cls / '*.png').resolve())))[:4]\n",
    "    for c, f in enumerate(files):\n",
    "        ax = axes[r, c]\n",
    "        ax.imshow(Image.open(f))\n",
    "        ax.set_title(f\"{cls} synth\", fontsize=8)\n",
    "        ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e43fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9 — Zip for portability\n",
    "import zipfile, os\n",
    "zip_path = Path(ARTIFACTS) / \"synthetic_patches.zip\"\n",
    "with zipfile.ZipFile(zip_path, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "    for cls in CLASSES:\n",
    "        for f in (Path(SYNTH_DIR)/cls).glob(\"*.png\"):\n",
    "            zf.write(f, arcname=f\"synthetic_patches/{cls}/{f.name}\")\n",
    "    zf.write(Path(ARTIFACTS)/\"synth_manifest.json\", arcname=\"artifacts/synth_manifest.json\")\n",
    "print(\"Zipped:\", zip_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
